// example call: time webppl model.wppl --require webppl-json --require webppl-sample-writer 1

var chain = last(process.argv)

// data


var levels = function(df, label) {
  return _.uniq(_.map(df, label));
}

    // helper functions

var foreach = function(fn, lst) {
      var foreach_ = function(i) {
        if (i < lst.length) {
          fn(lst[i]);
          foreach_(i + 1);
        }
      };
      foreach_(0);
    };

var logistic = function(x) {
      1 / (1 + Math.exp(-x))
    }

//////// data
var data = json.read('../data/data.json');
var tasks = levels(data, "task")
var subjects = levels(data, "id")

var meData = _.filter(data, {
  task: "mutual_exclusivity"})

var meItems = levels(meData, "item") // words

var infData = _.filter(data, {
  task: "simple_inf"})

var infItems = levels(infData, "item") // trials

var adhocData = _.filter(data, {
  task: "ad_hoc_implicature"})

////////////////////////////////////////
    // mutual exclusivity model utilities//

var all_me_objects = [
      { shape: "novel_object"},
      { shape: "familiar_object"}
    ]

var me_labels = ["novel_word", "familiar_word"]


var me_lexicon1 = function(utterance, obj, sem_knowledge) {
      utterance.label == "novel_word" ? obj.shape == "novel_object" :
      utterance.label == "familiar_word" ? flip(sem_knowledge) ?
      obj.shape == "familiar_object" :
      flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" :
      true
    }

  var me_lexicon2 = function(utterance, obj, sem_knowledge) {
      utterance.label == "novel_word" ? obj.shape == "familiar_object" :
      utterance.label == "familiar_word" ? flip(sem_knowledge) ?
      obj.shape == "familiar_object" :
      flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" :
      true
    }

    var meLexiconObjects = {
      "novel_word = novel_object": {
        novel_object: "novel_word",
        familiar_object: "familiar_word"
      },
      "novel_word = familiar_object": {
        novel_object: "familiar_word",
        familiar_object: "familiar_word"
      },
    }

    var meLexiconObject = {
      "novel_word = novel_object": me_lexicon1,
      "novel_word = familiar_object": me_lexicon2
    }

    var meUtterancePrior = function() {
      return uniformDraw([{
        label: "novel_word"
      }, {
        label: "familiar_word"
      }])
    }

    var meLexiconPrior = Categorical({
      vs: ["novel_word = novel_object", "novel_word = familiar_object"],
      ps: [1, 1]
    })

    //////////////////////////////////////
    // informativeness  model utilities//

    var inf_all_objects = [
      { shape: "triangle", id:1, location: 1},
      { shape: "triangle", id:2, location: 2},
      { shape: "circle", id:1, location: 2}
    ]

    // listener prior is over types, not tokens (we use uniformDraw on prior_objects)
    var inf_prior_objects = [
      { shape: "triangle", id:1, location: 1},
      { shape: "circle", id:1, location: 2}
    ]

    var inf_labels = ["dax","wug"]

    // two possible lexica, corresponding to meaning of words "dax" and "wug"
    var inf_lexicon1 = function(utterance, obj){
      utterance.label == "dax" ? obj.shape == "triangle" :
      utterance.label == "wug" ? obj.shape == "circle" :
      true
    }

    var inf_lexicon2 = function(utterance, obj){
      utterance.label == "dax" ? obj.shape == "circle" :
      utterance.label == "wug" ? obj.shape == "triangle" :
      true
    }

    var inf_lexiconObjects = {
      "dax = triangle": {
        triangle: "dax", circle: "wug"
      },
      "dax = circle": {
        triangle: "wug", circle: "dax"
      },
    }

    var inf_lexiconObject = {
      "dax = triangle": inf_lexicon1,
      "dax = circle" : inf_lexicon2
    }

    var point = function(utterance, obj){
      return obj.location == utterance.point
    }


    var infUtterancePrior = function(obj, lexiconName){
      var locationsWithShape = _.map(_.filter(inf_all_objects, {shape: obj.shape}), "location")
      var point = uniformDraw(locationsWithShape)
      var label = inf_lexiconObjects[lexiconName][obj.shape]
      return {label: label, point: point}
    }

    var infLexiconPrior = Categorical({vs: ["dax = triangle","dax = circle" ], ps: [1, 1]})

    ////////// model
    var model = function() {

      //////// semantic knowledge /////////////

      var globalLineParameters = {
        intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
        slope: uniformDrift({a: 0, b: 2, width: 0.5})
      }


      // ad prior for scale parameter

      var itemVariability = {
        intercept: uniformDrift({a: 0, b: 2, width: 0.2}),
        slope: uniformDrift({a: 0, b: 1, width: 0.2})
      }

      var sampleItemParameters = function(itemName) {
        return [itemName, {
          intercept: gaussianDrift({
            mu: globalLineParameters.intercept,
            sigma: itemVariability.intercept,
            width: 0.5
          }),
          slope: gaussianDrift({
            mu: globalLineParameters.slope,
            sigma: itemVariability.slope,
            width: 0.5
          })
        }]
      }

      var all_item_parameters = _.fromPairs(map(sampleItemParameters, meItems))

      //display(all_item_parameters)

      ////////////////  speaker optimality ////////////////////////

      var speakerOptimalityParameters = {
        intercept: uniformDrift({
          a: -3,
          b: 3,
          width: 0.5
        }),
        slope: uniformDrift({
          a: 0,
          b: 4,
          width: 0.5
        })
      }

      var subject_sigma = uniformDrift({a: 0, b:1, width: 0.1})


      var sampleSpeakerOptimality = function(so){
        return gaussianDrift({ mu: so, sigma: subject_sigma, width: 0.1 })
      }

      var infScale = logNormal({ mu: 0, sigma: 0.5 })
      var adhocScale = logNormal({ mu: 0, sigma: 0.5 })

      //////////////////////////////////////////////////////////////////
      //////////////// ME Model //////////////////////
      //////////////////////////////////////////////////////////////////


      foreach(function(subid){
        // display(meData)

        var meSubjectData = _.filter(meData, {id: subid})

        var subj_age = meSubjectData[0].c_age_month

        //  display(subj_age)

        //display(meData[0].c_age_month)
        //display(meSubjectData[0].c_age_month)

        var so = speakerOptimalityParameters.intercept  + speakerOptimalityParameters.slope * subj_age

        //display(speakerOptimalityParameters)
        //display(so)

        var subjectSpeakerOptimality = sampleSpeakerOptimality(so)

        //display(subjectSpeakerOptimality)

        // each row is a different item
        // foreach(function(item){
        //
        //   var meSubjectDataItem = _.filter(meSubjectData, {
        //     item: item
        //   })
        //   //display(meSubjectDataItem)
        //   var meSubjectDataItem_correct = _.map(meSubjectDataItem, "correct")
        //   //display(meSubjectDataItem_correct)
        //
        //   var itemLineParameters = all_item_parameters[item]
        //   //display(itemLineParameters)
        //   var sem_knowledge = logistic(itemLineParameters.intercept +
        //     itemLineParameters.slope * subj_age)
        //
        //     //display(sem_knowledge)
        //     var meLiteralListener = cache(function(utterance){
        //       Infer({method: "enumerate", model: function(){
        //         var lexiconName = sample(meLexiconPrior);
        //         var lexicon = meLexiconObject[lexiconName];
        //         var obj = sample( Categorical({vs: all_me_objects, ps: [.5,.5]}));
        //         if ("label" in utterance) {
        //           var truthValue = lexicon(utterance, obj, sem_knowledge);
        //           condition(truthValue)
        //         }
        //         return obj.shape
        //       }})}, 10000)
        //
        //       var meSpeaker = cache(function(obj, lexiconName, speakerOptimality){
        //         Infer({method: "enumerate", model: function(){
        //           var utterance = meUtterancePrior();
        //           var L0 = meLiteralListener(utterance);
        //           factor(speakerOptimality * L0.score(obj.shape))
        //           return utterance
        //         }})}, 10000)
        //
        //         var mePragmaticListener = function(utterance, speakerOptimality){
        //           Infer({method: "enumerate", model: function(){
        //             // display('inside RSA = ' + sem_knowledge)
        //             var lexiconName = sample(meLexiconPrior);
        //             var obj = sample( Categorical({vs: all_me_objects, ps: [.5,.5]}));
        //             var S1 = meSpeaker(obj, lexiconName, speakerOptimality);
        //             observe(S1, utterance)
        //             return obj.shape == "novel_object" ? 1 : 0
        //           }})}
        //
        //           var meModelPredictions = mePragmaticListener({label: "novel_word"}, subjectSpeakerOptimality)
        //           //var meModelPredictions = mePragmaticListener({label: "novel_word"}, so)
        //
        //
        //           mapData({
        //             data: meSubjectDataItem_correct
        //           }, function(d) {
        //             // display("me data " + meModelPredictions.score(d))
        //             observe(meModelPredictions, d)
        //           })
        //
        //         }, meItems)

            //////////////////////////////////////////////////////////////////
              ////////////////////// Inf model /////////////////////////////////
              //////////////////////////////////////////////////////////////////


                var infSubjectData = _.filter(infData, {id: subid})
                // display(infSubjectData)
                var infSubjectData_correct = _.map(infSubjectData, "correct")
                //display(infSubjectData_correct)
                var infSubjectSpeakerOptimality = subjectSpeakerOptimality * infScale

                //var infSpeakerOptimality = so * infScale
                var infSpeakerOptimality = so
                //display(infSpeakerOptimality)

                var infLiteralListener = cache(function(utterance){
                    Infer({method: "enumerate", model: function(){
                      var lexiconName = sample(infLexiconPrior);
                      var lexicon = inf_lexiconObject[lexiconName];
                      var obj = sample( Categorical({vs: inf_all_objects, ps: [.5, .5, .5] }));
                      if ("label" in utterance) {
                        var truthValue = lexicon(utterance, obj);
                        condition(truthValue)
                      }
                      if (utterance.point) {
                        var truthValuePoint = point(utterance, obj);
                        condition(truthValuePoint)
                      }
                      return obj.shape
                    }})
                  })

                  var infSpeaker = cache(function(obj, lexiconName, speakerOptimality){
                    Infer({method: "enumerate", model: function(){
                      var utterance = infUtterancePrior(obj, lexiconName);
                      var L0 = infLiteralListener(utterance, [.5, .5, .5] );
                      factor(speakerOptimality * L0.score(obj.shape))
                      return utterance
                    }})
                  })

                  var infPragmaticListener = cache(function(utterance, speakerOptimality){
                    Infer({method: "enumerate", model: function(){
                      var lexiconName = sample(infLexiconPrior);
                      var obj = sample( Categorical({vs: inf_all_objects, ps: [.5, .5, .5] }));
                      var S1 = infSpeaker(obj, lexiconName, speakerOptimality);
                      observe(S1, utterance)
                      return obj.shape == "circle" ? 1 : 0
                    }})
                  })


              //var infModelPredictions = infPragmaticListener({label: "dax", point: 2 }, infSubjectSpeakerOptimality)
              var infModelPredictions = infPragmaticListener({label: "dax", point: 2 }, infSpeakerOptimality)

              mapData({
                data: infSubjectData_correct
              }, function(d) {
                  //display("inf data " + infModelPredictions.score(d))
                observe(infModelPredictions, d)
              })


              //////////////////////////////////////////////////////////////////
                ////////////////////// AdhocImplicature model /////////////////////////////////
                //////////////////////////////////////////////////////////////////


                //   var adhocSubjectData = _.filter(adhocData, {id: subid})
                //    // display(adhocSubjectData)
                //   var adhocSubjectData_correct = _.map(adhocSubjectData, "correct")
                //    //display(adhocSubjectData_correct)
                //   var adhocSubjSpeakerOptimality = subjectSpeakerOptimality * adhocScale
                //   //display(adhocSubjSpeakerOptimality)
                //   var adhocSpeakerOptimality = so * adhocScale
                //
                //
                //   // utterance is
                //   var adhocLiteralListener = cache(function(utterance){
                //       Infer({method: "enumerate", model: function(){
                //         // here: utterance is expected to be the shape label ("triangle" or "circle")
                //         var location = uniformDraw([1, 2])
                //         var objectsOnTable = _.map(
                //           _.filter(inf_all_objects, {location: location}),
                //           "shape")
                //         // check if utterance(shape label) is contained in set of shape labels at location
                //         condition(objectsOnTable.indexOf(utterance) > -1 )
                //         return location
                //       }})
                //     })
                //
                //     var adhocSpeaker = cache(function(loc, speakerOptimality){
                //       Infer({method: "enumerate", model: function(){
                //         var utterance = uniformDraw(["triangle", "circle"])
                //         var L0 = adhocLiteralListener(utterance);
                //         factor(speakerOptimality * L0.score(loc))
                //         return utterance
                //       }})
                //     })
                //
                //     var adhocPragmaticListener = cache(function(utterance, speakerOptimality){
                //       Infer({method: "enumerate", model: function(){
                //         var location = uniformDraw([1, 2])
                //         var S1 = adhocSpeaker(location, speakerOptimality);
                //         observe(S1, utterance)
                //         return location == 1 ? 1 : 0
                //       }})
                //     })
                //
                //
                // //var adhocModelPredictions = adhocPragmaticListener("triangle", adhocSubjSpeakerOptimality)
                // var adhocModelPredictions = adhocPragmaticListener("triangle", adhocSpeakerOptimality)
                // //display(adhocModelPredictions)
                //
                //
                //
                //
                // mapData({
                //   data: adhocSubjectData_correct
                // }, function(d) {
                //    //display("ad hoc data " + adhocModelPredictions.score(d))
                //   observe(adhocModelPredictions, d)
                // })



              // foreach(function(item){
              //   var itemLineParameters = all_item_parameters[item]
              //   query.add(["semantic_knowledge", "item_parameter","intercept", item], itemLineParameters.intercept)
              //   query.add(["semantic_knowledge", "item_parameter","slope", item], itemLineParameters.slope)
              // }, meItems)

              //  query.add(["speaker_optimality","subj_parameter","speaker_optimality", subid], [subjectSpeakerOptimality])
              //  query.add(["speaker_optimality","subj_parameter","inf_speaker_optimality", subid], [infSubjectSpeakerOptimality])
              //  query.add(["speaker_optimality","subj_parameter","adhoc_speaker_optimality", subid], [adhocSubjSpeakerOptimality])


              }, subjects)


            //  query.add(["semantic_knowledge", "global_parameter","intercept","NA"], globalLineParameters.intercept)
            //  query.add(["semantic_knowledge", "global_parameter","slope","NA"], globalLineParameters.slope)

            //  query.add(["speaker_optimality","global_parameter","scale_parameter", "inf"], [infScale])
            //  query.add(["speaker_optimality","global_parameter","scale_parameter", "ad_hoc"], [adhocScale])
              query.add(["speaker_optimality","global_parameter", "intercept", "NA"], speakerOptimalityParameters.intercept)
              query.add(["speaker_optimality","global_parameter", "slope", "NA"], speakerOptimalityParameters.slope)
            //  query.add(["speaker_optimality","global_parameter", "sigma", "NA"], [subject_sigma])

              return query
            }


            var header = "iteration,a,b,c,d,e,f"

            var totalIterations = 50000 , lag =  2;
            // var totalIterations = 2, lag =  1;
            var samples = totalIterations/lag, burn = totalIterations / 2;


            var output_file = 'output/model-' + totalIterations + '_burn' + burn + '_lag' + lag + '_chain' + chain + '.csv'
            // //var output_file = 'output/model-test.csv'
            var callback = webpplSampleWriter.streamQueryCSV(output_file, header);

            var output = Infer({
              model,
              samples: samples,
              burn: burn,
              lag: lag,
              verbose: true,
              method: 'incrementalMH',
              //method: 'MCMC',
              verboseLag: 1000,
              onlyMAP: true,
              callbacks: [callback]
            });

            'output written to ' + output_file
